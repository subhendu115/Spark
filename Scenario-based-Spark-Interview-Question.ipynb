{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/subhendu115/Spark/blob/main/multiplt-delimeter.ipynb",
      "authorship_tag": "ABX9TyMCTkofyNfnM0MjmaWl4c6b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subhendu115/Spark/blob/main/Scenario-based-Spark-Interview-Question.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "AKTRtkx8GqzF",
        "outputId": "5b7a8b81-f77a-42ec-8006-5e61fcce4b2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ],
      "metadata": {
        "id": "rDCzH_-j_AsF",
        "outputId": "f094a3e8-8835-4716-9eb8-f16f7159d449",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "uYZjOW8h8A-Z"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"MultiDelim\").master(\"local[*]\").getOrCreate()\n",
        "import os;print(os.getcwd())"
      ],
      "metadata": {
        "id": "mF2MEgMoIYQd",
        "outputId": "b1379a0e-0787-4a46-b4a6-aa3e9f9d3746",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "basePath = \"/content/gdrive/MyDrive/SparkData/dataset-master/\""
      ],
      "metadata": {
        "id": "cVi4Txo6Hkee"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multiple Delimeter**"
      ],
      "metadata": {
        "id": "FpgCjjTOn7Rm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiple delimeter file\n",
        "df = spark.read.format(\"csv\").option(\"sep\",\"~|\").option(\"header\",True).load(basePath+\"/multi-delim.dat\")"
      ],
      "metadata": {
        "id": "ICPqHJlzInHS"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CShum70lJkWf",
        "outputId": "472f7edf-e137-4b01-d040-c65cfa8c7dae"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------+---+\n",
            "| id|       name|Age|\n",
            "+---+-----------+---+\n",
            "|  1|       gsfg| 23|\n",
            "|  1|  ssdgdsgjs| 20|\n",
            "|  1|svfgbfbggjs| 21|\n",
            "|  1|     svfscv| 29|\n",
            "|  1|   svcvvbjs| 23|\n",
            "|  1| ssgsrhjbjs| 28|\n",
            "|  1|  svfhfhbjs| 23|\n",
            "|  1|  svfdgdbjs| 25|\n",
            "|  1|   svfdgbjs| 22|\n",
            "|  1| svdgngjbjs| 25|\n",
            "+---+-----------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = spark.read.format(\"text\").option(\"header\",True).load(basePath+\"/multi-delim.dat\")"
      ],
      "metadata": {
        "id": "7Kt8ArY9J0c7"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwRDVE9wKlIJ",
        "outputId": "a4445369-97b4-430a-b18f-466cd7ca78f5"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+\n",
            "|             value|\n",
            "+------------------+\n",
            "|     id~|name~|Age|\n",
            "|       1~|gsfg~|23|\n",
            "|  1~|ssdgdsgjs~|20|\n",
            "|1~|svfgbfbggjs~|21|\n",
            "|     1~|svfscv~|29|\n",
            "|   1~|svcvvbjs~|23|\n",
            "| 1~|ssgsrhjbjs~|28|\n",
            "|  1~|svfhfhbjs~|23|\n",
            "|  1~|svfdgdbjs~|25|\n",
            "|   1~|svfdgbjs~|22|\n",
            "| 1~|svdgngjbjs~|25|\n",
            "+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df1.first())\n",
        "print(df1.first()[0]) # Row will have n number of indexes when dataframe have n columns. Here only one column"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTki-wV4KtpB",
        "outputId": "5acdf298-24d2-4d99-9645-b00cd1363fc7"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(value='id~|name~|Age')\n",
            "id~|name~|Age\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "header = df1.first()[0]\n",
        "header\n",
        "schm = header.split('~|')\n",
        "schm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LB-5TiTWWwxs",
        "outputId": "20b91954-ae5d-446f-99cf-dede70e860e2"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['id', 'name', 'Age']"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_in = df1.filter(df1['value'] != header).rdd.map(lambda row:row[0].split(\"~|\")).toDF(schm)\n",
        "df_in.show()\n",
        "# As the DataFarme have only One column -> hence row[0]. It can change based on number of column in dataframe/RDD"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4g2WI3ACL0qH",
        "outputId": "4e902205-1997-4c0c-82f6-43e25d09b5e5"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------+---+\n",
            "| id|       name|Age|\n",
            "+---+-----------+---+\n",
            "|  1|       gsfg| 23|\n",
            "|  1|  ssdgdsgjs| 20|\n",
            "|  1|svfgbfbggjs| 21|\n",
            "|  1|     svfscv| 29|\n",
            "|  1|   svcvvbjs| 23|\n",
            "|  1| ssgsrhjbjs| 28|\n",
            "|  1|  svfhfhbjs| 23|\n",
            "|  1|  svfdgdbjs| 25|\n",
            "|  1|   svfdgbjs| 22|\n",
            "|  1| svdgngjbjs| 25|\n",
            "+---+-----------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Merge two dataframe with different fields**"
      ],
      "metadata": {
        "id": "_--WAXg3oG_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Different Schema data merge\n",
        "df1 = spark.read.format('csv').option('header',True).load(basePath+\"input.csv\")\n",
        "df2 = spark.read.format('csv').option('header',True).load(basePath+\"input1.csv\")"
      ],
      "metadata": {
        "id": "ePlLk7Gv50je"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.show()\n",
        "df2.show()"
      ],
      "metadata": {
        "id": "tNIFURrkH9vy",
        "outputId": "8f2b1527-852e-4e77-9b55-5840e3013526",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+\n",
            "|     Name|Age|\n",
            "+---------+---+\n",
            "|bsfbssdds| 28|\n",
            "|  bsfbssd| 25|\n",
            "|  bsfbssd| 26|\n",
            "|   bsfbss| 20|\n",
            "|  bsfbsdf| 25|\n",
            "|   bsfbss| 26|\n",
            "|   bsfbsd| 23|\n",
            "+---------+---+\n",
            "\n",
            "+---------+---+------+\n",
            "|     Name|Age|Gender|\n",
            "+---------+---+------+\n",
            "|bsfbssdds| 28|     M|\n",
            "|  bsfbssd| 25|     F|\n",
            "|  bsfbssd| 26|     M|\n",
            "|   bsfbss| 20|     M|\n",
            "|  bsfbsdf| 25|     M|\n",
            "|   bsfbss| 26|     F|\n",
            "|   bsfbsd| 23|     M|\n",
            "+---------+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list1 = list(set(df1.columns) - set(df2.columns))\n",
        "list2 = list(set(df2.columns) - set(df1.columns))\n",
        "print(list1)\n",
        "print(list2)"
      ],
      "metadata": {
        "id": "tZSZFe89ICVp",
        "outputId": "7430c84f-ec10-4ad4-8da4-1a648ef69289",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "['Gender']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lit\n",
        "for col in list1:\n",
        "  df2 = df2.withColumn(col,lit(None))\n",
        "for col in list2:\n",
        "  df1 = df1.withColumn(col,lit(None))"
      ],
      "metadata": {
        "id": "mmuD7Ax8IaIV"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df1.unionAll(df2)\n",
        "df.show()"
      ],
      "metadata": {
        "id": "KVkUS0KsJJSP",
        "outputId": "4a3793c8-b30f-4002-c82a-e37b2b48c4bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+------+\n",
            "|     Name|Age|Gender|\n",
            "+---------+---+------+\n",
            "|bsfbssdds| 28|  NULL|\n",
            "|  bsfbssd| 25|  NULL|\n",
            "|  bsfbssd| 26|  NULL|\n",
            "|   bsfbss| 20|  NULL|\n",
            "|  bsfbsdf| 25|  NULL|\n",
            "|   bsfbss| 26|  NULL|\n",
            "|   bsfbsd| 23|  NULL|\n",
            "|bsfbssdds| 28|     M|\n",
            "|  bsfbssd| 25|     F|\n",
            "|  bsfbssd| 26|     M|\n",
            "|   bsfbss| 20|     M|\n",
            "|  bsfbsdf| 25|     M|\n",
            "|   bsfbss| 26|     F|\n",
            "|   bsfbsd| 23|     M|\n",
            "+---------+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Dataframe from a file that contains 5 pipe delimited fields but in a single line**"
      ],
      "metadata": {
        "id": "AJn0_rvmoTFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format('csv').option('header',False).load(basePath+\"same_line_input.txt\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_ISEtmdYCNU",
        "outputId": "ae3ef5b5-eb29-4830-ebc7-27b7412b5df6"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|                 _c0|\n",
            "+--------------------+\n",
            "|Azar|BE|Bigdata|9...|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import regexp_replace,split,explode,col"
      ],
      "metadata": {
        "id": "fckSXrvKahAT"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df.withColumn('chk',regexp_replace('_c0',\"(.*?\\\\|){4}\",\"$0-\"))\n",
        "df1.show(20,False)\n",
        "df1 = df1.withColumn('col_explode',explode(split('chk','\\\\|-'))).select('col_explode')\n",
        "df1.show(20,False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_KcRB0Pa7UA",
        "outputId": "993679ea-bc57-4845-a5e5-65c173573dc8"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+\n",
            "|_c0                                                                                               |chk                                                                                                   |\n",
            "+--------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+\n",
            "|Azar|BE|Bigdata|985632|bhshds|msc|cs|6666|shdj|BE|ML|64456|hksdh|BE|FSD|6466|jsda|Bsc|Bigdata|6444|Azar|BE|Bigdata|985632|-bhshds|msc|cs|6666|-shdj|BE|ML|64456|-hksdh|BE|FSD|6466|-jsda|Bsc|Bigdata|6444|\n",
            "+--------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "+----------------------+\n",
            "|col_explode           |\n",
            "+----------------------+\n",
            "|Azar|BE|Bigdata|985632|\n",
            "|bhshds|msc|cs|6666    |\n",
            "|shdj|BE|ML|64456      |\n",
            "|hksdh|BE|FSD|6466     |\n",
            "|jsda|Bsc|Bigdata|6444 |\n",
            "+----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = df1.select('col_explode').rdd.map(lambda x:x[0].split('|'))\n",
        "#rdd = df1.withColumn('Name',split(df1['col_explode'],'\\\\|')) // Dynamic column name is difficult with dataframe\n",
        "rdd.collect()\n",
        "df2 = rdd.toDF(['Name','Degree','Subject','Contact'])\n",
        "df2.show(20,False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5ILR1zNct_v",
        "outputId": "4e6f8582-be15-46d3-b9c4-b6bb3f426960"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-------+-------+\n",
            "|Name  |Degree|Subject|Contact|\n",
            "+------+------+-------+-------+\n",
            "|Azar  |BE    |Bigdata|985632 |\n",
            "|bhshds|msc   |cs     |6666   |\n",
            "|shdj  |BE    |ML     |64456  |\n",
            "|hksdh |BE    |FSD    |6466   |\n",
            "|jsda  |Bsc   |Bigdata|6444   |\n",
            "+------+------+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample Explode, Explode_outer,PosExplode, PosExplode_Outer**"
      ],
      "metadata": {
        "id": "sYePCqFQor1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format('csv').option('Sep','|').option('header',True).load(basePath+\"input4.csv\")\n",
        "df.show(200,False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXDumEiqp55O",
        "outputId": "9863ca98-087e-4904-b48d-276734e27660"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+-----------------+\n",
            "|Name |Age|Subject          |\n",
            "+-----+---+-----------------+\n",
            "|hbsbc|23 |ssf,fffs,dfdf,sdf|\n",
            "|wwd  |43 |ssf,fffs,sdf     |\n",
            "|rer  |33 |dfdf,sdf         |\n",
            "|ewer |23 |ssf,fffs,dfdf,   |\n",
            "|dwsf |23 |NULL             |\n",
            "+-----+---+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import split,explode_outer,posexplode,posexplode_outer\n",
        "\n",
        "df1 = df.withColumn('Qualification',explode(split('Subject','\\\\,')))\n",
        "# NULL not considered\n",
        "df1.show(200,False)\n",
        "df2 = df.withColumn('Qualification',explode_outer(split('Subject','\\\\,')))\n",
        "# NULL Considered\n",
        "df2.show(200,False)\n",
        "# NULL not considered and extra position field\n",
        "df3 = df.select('*',posexplode(split('Subject','\\\\,')))\n",
        "# NULL Considered\n",
        "df3.show(200,False)\n",
        "# NULL  considered and extra position field\n",
        "df4 = df.select('*',posexplode_outer(split('Subject','\\\\,')))\n",
        "# NULL Considered\n",
        "df4.show(200,False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGsYWVliqAye",
        "outputId": "2142d920-7619-49f7-f438-9e95b0fa1dad"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+-----------------+-------------+\n",
            "|Name |Age|Subject          |Qualification|\n",
            "+-----+---+-----------------+-------------+\n",
            "|hbsbc|23 |ssf,fffs,dfdf,sdf|ssf          |\n",
            "|hbsbc|23 |ssf,fffs,dfdf,sdf|fffs         |\n",
            "|hbsbc|23 |ssf,fffs,dfdf,sdf|dfdf         |\n",
            "|hbsbc|23 |ssf,fffs,dfdf,sdf|sdf          |\n",
            "|wwd  |43 |ssf,fffs,sdf     |ssf          |\n",
            "|wwd  |43 |ssf,fffs,sdf     |fffs         |\n",
            "|wwd  |43 |ssf,fffs,sdf     |sdf          |\n",
            "|rer  |33 |dfdf,sdf         |dfdf         |\n",
            "|rer  |33 |dfdf,sdf         |sdf          |\n",
            "|ewer |23 |ssf,fffs,dfdf,   |ssf          |\n",
            "|ewer |23 |ssf,fffs,dfdf,   |fffs         |\n",
            "|ewer |23 |ssf,fffs,dfdf,   |dfdf         |\n",
            "|ewer |23 |ssf,fffs,dfdf,   |             |\n",
            "+-----+---+-----------------+-------------+\n",
            "\n",
            "+-----+---+-----------------+-------------+\n",
            "|Name |Age|Subject          |Qualification|\n",
            "+-----+---+-----------------+-------------+\n",
            "|hbsbc|23 |ssf,fffs,dfdf,sdf|ssf          |\n",
            "|hbsbc|23 |ssf,fffs,dfdf,sdf|fffs         |\n",
            "|hbsbc|23 |ssf,fffs,dfdf,sdf|dfdf         |\n",
            "|hbsbc|23 |ssf,fffs,dfdf,sdf|sdf          |\n",
            "|wwd  |43 |ssf,fffs,sdf     |ssf          |\n",
            "|wwd  |43 |ssf,fffs,sdf     |fffs         |\n",
            "|wwd  |43 |ssf,fffs,sdf     |sdf          |\n",
            "|rer  |33 |dfdf,sdf         |dfdf         |\n",
            "|rer  |33 |dfdf,sdf         |sdf          |\n",
            "|ewer |23 |ssf,fffs,dfdf,   |ssf          |\n",
            "|ewer |23 |ssf,fffs,dfdf,   |fffs         |\n",
            "|ewer |23 |ssf,fffs,dfdf,   |dfdf         |\n",
            "|ewer |23 |ssf,fffs,dfdf,   |             |\n",
            "|dwsf |23 |NULL             |NULL         |\n",
            "+-----+---+-----------------+-------------+\n",
            "\n",
            "+-----+---+-----------------+---+----+\n",
            "|Name |Age|Subject          |pos|col |\n",
            "+-----+---+-----------------+---+----+\n",
            "|hbsbc|23 |ssf,fffs,dfdf,sdf|0  |ssf |\n",
            "|hbsbc|23 |ssf,fffs,dfdf,sdf|1  |fffs|\n",
            "|hbsbc|23 |ssf,fffs,dfdf,sdf|2  |dfdf|\n",
            "|hbsbc|23 |ssf,fffs,dfdf,sdf|3  |sdf |\n",
            "|wwd  |43 |ssf,fffs,sdf     |0  |ssf |\n",
            "|wwd  |43 |ssf,fffs,sdf     |1  |fffs|\n",
            "|wwd  |43 |ssf,fffs,sdf     |2  |sdf |\n",
            "|rer  |33 |dfdf,sdf         |0  |dfdf|\n",
            "|rer  |33 |dfdf,sdf         |1  |sdf |\n",
            "|ewer |23 |ssf,fffs,dfdf,   |0  |ssf |\n",
            "|ewer |23 |ssf,fffs,dfdf,   |1  |fffs|\n",
            "|ewer |23 |ssf,fffs,dfdf,   |2  |dfdf|\n",
            "|ewer |23 |ssf,fffs,dfdf,   |3  |    |\n",
            "+-----+---+-----------------+---+----+\n",
            "\n",
            "+-----+---+-----------------+----+----+\n",
            "|Name |Age|Subject          |pos |col |\n",
            "+-----+---+-----------------+----+----+\n",
            "|hbsbc|23 |ssf,fffs,dfdf,sdf|0   |ssf |\n",
            "|hbsbc|23 |ssf,fffs,dfdf,sdf|1   |fffs|\n",
            "|hbsbc|23 |ssf,fffs,dfdf,sdf|2   |dfdf|\n",
            "|hbsbc|23 |ssf,fffs,dfdf,sdf|3   |sdf |\n",
            "|wwd  |43 |ssf,fffs,sdf     |0   |ssf |\n",
            "|wwd  |43 |ssf,fffs,sdf     |1   |fffs|\n",
            "|wwd  |43 |ssf,fffs,sdf     |2   |sdf |\n",
            "|rer  |33 |dfdf,sdf         |0   |dfdf|\n",
            "|rer  |33 |dfdf,sdf         |1   |sdf |\n",
            "|ewer |23 |ssf,fffs,dfdf,   |0   |ssf |\n",
            "|ewer |23 |ssf,fffs,dfdf,   |1   |fffs|\n",
            "|ewer |23 |ssf,fffs,dfdf,   |2   |dfdf|\n",
            "|ewer |23 |ssf,fffs,dfdf,   |3   |    |\n",
            "|dwsf |23 |NULL             |NULL|NULL|\n",
            "+-----+---+-----------------+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Dataframe from Json**"
      ],
      "metadata": {
        "id": "DpfW9Ie2o5ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format('csv').option('header',True).option('escape','\\\"').option('multiline',True).load(basePath+\"dummy2.csv\")\n",
        "df.show(200,False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYNAWcaqtIWY",
        "outputId": "9bcbff81-b55d-4654-e0f1-dd57d6d3e473"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+--------------+-------------------------------------------------------------------------------+\n",
            "|PartitionDate|Status        |request                                                                        |\n",
            "+-------------+--------------+-------------------------------------------------------------------------------+\n",
            "|2020-06-30   |Internal Error|{\"Response\":{\"MessageId\" : 15432 }}                                            |\n",
            "|2020-06-30   |Success       |{\"Response\":{\"MessageId\" : 15432,\"Latitude\":\"-176.2989\",\"longitude\":\"7.3614\" }}|\n",
            "+-------------+--------------+-------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col,json_tuple,to_json,from_json"
      ],
      "metadata": {
        "id": "EYGlnzpDuHql"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df.select('*',json_tuple('request','Response'))\n",
        "df2 = df1.select('*',json_tuple('c0','MessageId','Latitude','longitude').alias('MessageId','Latitude','longitude')).drop('request','c0')\n",
        "df2.show(200,False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-NZprkkuUsQ",
        "outputId": "33a53e95-d76d-4c1b-9249-16d79def4de6"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+--------------+---------+---------+---------+\n",
            "|PartitionDate|Status        |MessageId|Latitude |longitude|\n",
            "+-------------+--------------+---------+---------+---------+\n",
            "|2020-06-30   |Internal Error|15432    |NULL     |NULL     |\n",
            "|2020-06-30   |Success       |15432    |-176.2989|7.3614   |\n",
            "+-------------+--------------+---------+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove first 7 lines and create dataframe from remaining records**"
      ],
      "metadata": {
        "id": "PiNncWQnpJ1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inp = spark.sparkContext.textFile(basePath+'pageview.csv',2).map(lambda x:x.split(','))\n",
        "print(inp.getNumPartitions())\n",
        "inp.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Db_wdWrivtBv",
        "outputId": "9b21be49-8206-4789-f64d-3ed44ec8b468"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Site', 'www.learntospark.com'],\n",
              " ['Desccription', '\"Complete guide to learn Spark', 'AI', 'ML\"'],\n",
              " ['Page Views of each blog'],\n",
              " ['20200817-20200817'],\n",
              " [''],\n",
              " ['Total data in page', '12'],\n",
              " [''],\n",
              " ['Page', 'Date', 'Pageviews', 'Unique_Pageviews', 'Sessions'],\n",
              " ['Guide to Install Spark', '2020-08-17', '1140', '986', '800'],\n",
              " ['Spark MAP vs FlatMap', '2020-08-17', '836', '800', '128'],\n",
              " ['Spark Architechture', '2020-08-17', '1569', '1345', '1400'],\n",
              " ['Azure Function for Mp3 to text', '2020-08-17', '350', '245', '234'],\n",
              " ['Scala Vs Python', '2020-08-17', '200', '150', '130'],\n",
              " ['Spark Window Function', '2020-08-17', '789', '546', '560'],\n",
              " ['Natural Language Processing', '2020-08-17', '467', '456', '100'],\n",
              " ['Spark Linear Interpolation - Time Series',\n",
              "  '2020-08-17',\n",
              "  '698',\n",
              "  '345',\n",
              "  '349'],\n",
              " ['Spark case statement', '2020-08-17', '234', '196', '120'],\n",
              " ['Spark Scenario Based Questions', '2020-08-17', '712', '329', '137'],\n",
              " ['Spark v3.0 Delta Lake', '2020-08-17', '333', '198', '39'],\n",
              " ['Screen Recorder App using Python', '2020-08-17', '766', '567', '344'],\n",
              " ['Spark trick with Show()', '2020-08-17', '108', '35', '24'],\n",
              " ['Spark Cache() Vs Persist', '2020-08-17', '587', '432', '300'],\n",
              " ['Image Processing text to audio', '2020-08-17', '384', '123', '84']]"
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_drop = inp.mapPartitionsWithIndex(lambda id_x,iter:list(iter)[8:] if (id_x == 0) else iter)\n",
        "rdd_drop.collect()\n",
        "schm = ['Page','Date','PageViews','Uniq_views','Session']\n",
        "# rdd_drop.toDF().show()\n",
        "out_df = spark.createDataFrame(rdd_drop,schm)\n",
        "out_df.show(200,False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyGWkIDBhxct",
        "outputId": "d4826779-6d47-4fbb-8ae2-ac85f81a9073"
      },
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------+----------+---------+----------+-------+\n",
            "|Page                                    |Date      |PageViews|Uniq_views|Session|\n",
            "+----------------------------------------+----------+---------+----------+-------+\n",
            "|Guide to Install Spark                  |2020-08-17|1140     |986       |800    |\n",
            "|Spark MAP vs FlatMap                    |2020-08-17|836      |800       |128    |\n",
            "|Spark Architechture                     |2020-08-17|1569     |1345      |1400   |\n",
            "|Azure Function for Mp3 to text          |2020-08-17|350      |245       |234    |\n",
            "|Scala Vs Python                         |2020-08-17|200      |150       |130    |\n",
            "|Spark Window Function                   |2020-08-17|789      |546       |560    |\n",
            "|Natural Language Processing             |2020-08-17|467      |456       |100    |\n",
            "|Spark Linear Interpolation - Time Series|2020-08-17|698      |345       |349    |\n",
            "|Spark case statement                    |2020-08-17|234      |196       |120    |\n",
            "|Spark Scenario Based Questions          |2020-08-17|712      |329       |137    |\n",
            "|Spark v3.0 Delta Lake                   |2020-08-17|333      |198       |39     |\n",
            "|Screen Recorder App using Python        |2020-08-17|766      |567       |344    |\n",
            "|Spark trick with Show()                 |2020-08-17|108      |35        |24     |\n",
            "|Spark Cache() Vs Persist                |2020-08-17|587      |432       |300    |\n",
            "|Image Processing text to audio          |2020-08-17|384      |123       |84     |\n",
            "+----------------------------------------+----------+---------+----------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_wxVAp2lnm-j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}